{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3Ft0VrUcO6H71UGUvLX7B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waleed-durrani/gen-ai-360/blob/main/GenAI_360_LangChain_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Gen AI 360**: Foundational Model Certification ✈\n",
        "\n",
        "## **LangChain & Vector Databases in Production**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "7taLjBh-nI2d"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iD2ya8CvpsrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Required API Tokens:\n",
        "\n",
        "- [OpenAI API Token](https://platform.openai.com/playground): Create the account and generate API Key.\n",
        "- [Deep Lake API Token](https://app.activeloop.ai/register): Create the account and generate API Key.\n",
        "\n",
        "**Note:** Use .env file to store the API keys. And then save the file in Google Drive, mount it, and then use it. Or directly upload it without mounting Google Drive."
      ],
      "metadata": {
        "id": "n_46TTNbpyb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install virtualenv"
      ],
      "metadata": {
        "id": "LmMnam3TKk_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!virtualenv venv"
      ],
      "metadata": {
        "id": "UENuM7K2Kt6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!source venv/bin/activate"
      ],
      "metadata": {
        "id": "UPdiL4u2K6rG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dotenv package to work with .env(Environmental Variables):"
      ],
      "metadata": {
        "id": "zHv-zOhHaMfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "id": "ktpeX421-uwx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efae72ea-ce5e-4ad3-d94e-38b7ca3e3a4b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Required Packages:\n",
        "\n",
        "Try using latest versions of the following packages except langchain. For smooth practical, try using specified version of LangChain:\n",
        "\n",
        "- langchain==0.0.208 (Course's code is tested on this version)\n",
        "- deeplake\n",
        "- openai\n",
        "- tiktoken\n",
        "\n",
        "`!pip install langchain==0.0.208 deeplake openai tiktoken`\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "WuwYwFJZg8bM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the following code to install required packages:"
      ],
      "metadata": {
        "id": "0OeT2CXu4rE8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Rml295efIrJ"
      },
      "outputs": [],
      "source": [
        "!pip install langchain==0.0.208 deeplake openai tiktoken -qq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())\n",
        "\n",
        "openai.api_key  = os.getenv('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "ztPc8dW2_m7W"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LangChain's Fundamental Concept: Invoking an LLM with a Specific Input:"
      ],
      "metadata": {
        "id": "KpS_7_-V0bZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing required modules\n",
        "import langchain\n",
        "import openai\n",
        "import tiktoken\n",
        "import deeplake\n",
        "\n",
        "# importing langchain's OpenAI class\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "QvnVEMvlJTcp"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# connect OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
        "# using one of GPT-3's variants, called text-davinci-003.\n",
        "# higher temperature = more diverse and unexpected text (can also means more error)\n",
        "\n",
        "llm = OpenAI(model=\"text-davinci-003\", temperature=0.7)"
      ],
      "metadata": {
        "id": "CmVoOjV7NCKc"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Suggest a personalized workout routine for someone looking to improve cardiovascular endurance and prefers outdoor activities.\"\n",
        "print(llm(text))"
      ],
      "metadata": {
        "id": "XYlW-y29eU27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using a different variant of GPT-3, called text-curie-001.\n",
        "llm = OpenAI(model=\"text-curie-001\", temperature=0.3)"
      ],
      "metadata": {
        "id": "4F6THyLegxrc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt Design Idea from \"ChatGPT Prompt Engineering for Developers\" by DeepLearning.AI and OpenAI\n",
        "\n",
        "text = f\"\"\"Hey, I am a good human being. You got that? And one more thing, you need to understand the\n",
        "consequences before you spit out something from your mouth\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"Use the professional tone to turn text delimited by triple backticks into formal english.\n",
        "```{text}```\n",
        "\"\"\"\n",
        "\n",
        "print(llm(prompt))"
      ],
      "metadata": {
        "id": "LNnE3DlsrNwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chains in LangChain"
      ],
      "metadata": {
        "id": "SzLWPazX5mVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Company name generation based on products they sell"
      ],
      "metadata": {
        "id": "VuFgHyr8Itse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# specifying LLM settings (which llm, and the degree of randomness), using OpenAI class from LangChain\n",
        "llm = OpenAI(model=\"text-davinci-003\", temperature=0.9)\n",
        "\n",
        "# langchain's PromtTemplate class: To create dynamic prompts that incorporate user input.\n",
        "# define a template with input variables and fill them with actual values when generating the prompt\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good name for a company that makes {product}?\",\n",
        ")\n",
        "\n",
        "# takes LLM Object (Chat Model or LLM), and PromptTemplate object\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "\n",
        "# Run the chain only specifying the input variable.\n",
        "print(chain.run(\"Eco-friendly makeup\"))\n",
        "\n",
        "# print(chain.prompt.input_variables[0])"
      ],
      "metadata": {
        "id": "ZaYf9Tiyfhhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Writes a story using inputs (genre, character, setting) from users"
      ],
      "metadata": {
        "id": "skvztwXCMnRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# specifying LLM settings (which llm, and the degree of randomness), using OpenAI class from LangChain\n",
        "llm = OpenAI(model=\"text-davinci-003\", temperature=0.9)\n",
        "\n",
        "# langchain's PromtTemplate class: To create dynamic prompts that incorporate user input.\n",
        "# define a template with input variables and fill them with actual values when generating the prompt\n",
        "prompt = PromptTemplate(\n",
        "\n",
        "    input_variables=[\"genre\", \"character\", \"setting\"],\n",
        "\n",
        "    template=\"Write a {genre} story about {character}, and {setting}. Use 50 words at the most.\",\n",
        ")\n",
        "\n",
        "\n",
        "# takes LLM Object (Chat Model or LLM), and PromptTemplate object\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Use the input() function to take input from the user for each input variable\n",
        "genre = input(\"Enter a genre: \")\n",
        "character = input(\"Enter a character: \")\n",
        "setting = input(\"Enter a setting: \")\n",
        "\n",
        "\n",
        "# Use the run() method of the LLMChain object to execute the chain and generate the output\n",
        "output = chain.run(\n",
        "    genre=genre,\n",
        "    character=character,\n",
        "    setting=setting\n",
        ")\n",
        "\n",
        "# Print the output\n",
        "print(output)\n",
        "\n",
        "# Run the chain only specifying the input variable.\n",
        "# print(chain.run(\"Eco-friendly makeup\"))"
      ],
      "metadata": {
        "id": "qZe98aL872Te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conversation memory in LangChain"
      ],
      "metadata": {
        "id": "Wucx5WZDQSjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    memory=ConversationBufferMemory()\n",
        ")\n",
        "\n",
        "# Start the conversation\n",
        "conversation.predict(input=\"Tell me about yourself.\")\n",
        "\n",
        "# Continue the conversation\n",
        "conversation.predict(input=\"What can you do?\")\n",
        "conversation.predict(input=\"How can you help me with data analysis?\")\n",
        "\n",
        "# Display the conversation\n",
        "print(conversation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQ92ng07o6GA",
        "outputId": "a8325659-15ef-4997-8eae-c97838ef12c3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Tell me about yourself.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Tell me about yourself.\n",
            "AI:  Hi there! I'm an AI created to help people with their daily tasks. I'm programmed to understand natural language and respond to questions and commands. I'm also able to learn from my interactions with people, so I'm constantly growing and improving. I'm excited to help you out!\n",
            "Human: What can you do?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Tell me about yourself.\n",
            "AI:  Hi there! I'm an AI created to help people with their daily tasks. I'm programmed to understand natural language and respond to questions and commands. I'm also able to learn from my interactions with people, so I'm constantly growing and improving. I'm excited to help you out!\n",
            "Human: What can you do?\n",
            "AI:  I can help you with a variety of tasks, such as scheduling appointments, setting reminders, and providing information. I'm also able to answer questions and provide advice. I'm always learning, so I'm sure I can help you with whatever you need.\n",
            "Human: How can you help me with data analysis?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "memory=ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Tell me about yourself.', additional_kwargs={}, example=False), AIMessage(content=\" Hi there! I'm an AI created to help people with their daily tasks. I'm programmed to understand natural language and respond to questions and commands. I'm also able to learn from my interactions with people, so I'm constantly growing and improving. I'm excited to help you out!\", additional_kwargs={}, example=False), HumanMessage(content='What can you do?', additional_kwargs={}, example=False), AIMessage(content=\" I can help you with a variety of tasks, such as scheduling appointments, setting reminders, and providing information. I'm also able to answer questions and provide advice. I'm always learning, so I'm sure I can help you with whatever you need.\", additional_kwargs={}, example=False), HumanMessage(content='How can you help me with data analysis?', additional_kwargs={}, example=False), AIMessage(content=\" I'm not currently able to help with data analysis, but I'm always learning and expanding my capabilities. I'm sure I'll be able to help you with data analysis in the future.\", additional_kwargs={}, example=False)]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='history') callbacks=None callback_manager=None verbose=True tags=None prompt=PromptTemplate(input_variables=['history', 'input'], output_parser=None, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:', template_format='f-string', validate_template=True) llm=OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all') output_key='response' output_parser=NoOpOutputParser() return_final_only=True llm_kwargs={} input_key='input'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZN7auvdlz-dc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}